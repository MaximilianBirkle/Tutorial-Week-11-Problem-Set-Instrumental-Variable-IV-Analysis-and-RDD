---
title: "Tutorial Week 11: Instrumental Variable (IV) Analysis and RDD"
author:
  - Maximilian Birkle
  - Daniel Lehmann
  - Henry Lucas
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    number_sections: true
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    code_folding: show
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  cache = FALSE
)
```

# Setup and Package Loading

```{r load-packages}
# Install packages if needed
packages <- c("AER", "haven", "dplyr", "ggplot2", "stargazer",
              "boot", "lmtest", "sandwich", "knitr", "kableExtra",
              "rdrobust", "rddensity", "rdlocrand")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}
```

# Load Data

```{r load-data}
# Load the AJR (2001) replication data
ajr_data_full <- haven::read_dta("acemoglu.dta")
cat("Observations:", nrow(ajr_data_full), "\n")
cat("Variables:", ncol(ajr_data_full), "\n")

```

# Problem 1: Instrumental Variable Analysis

## Q1. Writing and Estimating the IV Model (10 pts)

1. **Write down the two-equation IV system** (first stage and second stage).
    
    Define explicitly:
    
    - $Y_i$: outcome
    - $T_i$: endogenous regressor
    - $Z_i$: instrument
    - $X_i$: controls

2. Estimate the following:
    - **First stage:**
        
        $$T_i = \alpha_1 + \beta_1 Z_i + \mathbf{X}_i' \gamma_1 + \varepsilon_{1i}$$
        
    - **Reduced form:**
        
        $$Y_i = \alpha_2 + \delta_1 Z_i + \mathbf{X}_i' \gamma_2 + \varepsilon_{2i}$$
        
    - **2SLS:**
        
        $$Y_i = \alpha_3 + \beta_2 T_i + \mathbf{X}_i' \gamma_3 + \varepsilon_{3i}$$
        
3. Report and interpret:
    - The reduced-form coefficient on $Z_i$.
    - The first-stage coefficient on $Z_i$.
    - The **first-stage F-statistic**. Is it above the rule-of-thumb threshold of $\approx$ 10?
    - The 2SLS estimate of the effect of $T_i$ on $Y_i$. Is it statistically significant?
    
```{r q1_estimating_iv_model, results='asis'}

# Clean dataset with complete cases
ajr_data <- subset(ajr_data_full, baseco == 1)
  

# We define our control variables as a string for an easier formula building
controls <- "lat_abst + f_french + f_brit + sjlofr + catho80 + muslim80 + no_cpm80"

# First Stage: (Z_i -> T_i): We regress Endogeneous (avexpr) on Instrument (logem4) + Controls
fs_formula <- as.formula(paste("avexpr ~ logem4 +", controls))
fs_model <- lm(fs_formula, data = ajr_data)

# Reduced Form: (Z_i -> Y_i): We regress Outcome (logpgp95) on Instrument (logem4) + Controls
rf_formula <- as.formula(paste("logpgp95 ~ logem4 +", controls))
rf_model <- lm(rf_formula, data = ajr_data)

# 2SLS: (T_i -> Y_i): We regress Outcome (logpgp95) on Endogeneous (avexpr) + Controls
# For this stage, we have to use the ivreg-command. Since T_i is endogeneous (Y -> X),
# we need to run this command in order to isolate the specific variation in institutions
# caused by historical mortality rates and uses only that chunk of variation to explain GDP
iv_formula <- as.formula(paste("logpgp95 ~ avexpr +", controls, "| logem4 +", controls))
iv_model <- ivreg(iv_formula, data = ajr_data)

# For an instrumental variable to work, we have to verify that our instrumental variable Z_i
# is not a 'weak instrument'. Therefore, the instrument (Mortality, Z_i) must trigger a 
# significant change in the endogeneous variable (Institutions, T_i)
f_test <- linearHypothesis(fs_model, "logem4 = 0")
f_stat <- f_test$F[2]

# Output results using stargazer
stargazer(fs_model, rf_model, iv_model, 
          type = "latex", 
          title = "IV Analysis Results",
          column.labels = c("First Stage", "Reduced Form", "2SLS"),
          dep.var.labels = c("Exprop. Risk (T)", "Log GDP (Y)", "Log GDP (Y)"),
          covariate.labels = c("Log Settler Mortality (Z)", "Exprop. Risk (T)"),
          add.lines = list(c("First-Stage F-stat", round(f_stat, 2), "-", "-")))
```


## Q2. Randomization and Resampling (10 pts)

### Q2(a). Permutation Test

1. **State the null hypothesis** being tested.

2. Conduct a **permutation test** for the IV coefficient:
    - Shuffle the endogenous variable (or fitted values) while holding other variables fixed.
    - Re-estimate the IV model for each permutation.
    - Construct the empirical null distribution.
    - Report the **permutation p-value**.

3. Compare this p-value to the **normal-approximation p-value** from your 2SLS output.
    
    Discuss any differences and what they imply for small-sample IV inference.
    
```{r q2_permutation_test}

# Permutation Test

set.seed(123)
n_perms <- 1000
observed_coef <- coef(iv_model)["avexpr"]
perm_coefs <- numeric(n_perms)

for (i in 1:n_perms) {
  # Shuffle the endogeneous variable
  ajr_perm <- ajr_data
  ajr_perm$avexpr <- sample(ajr_data$avexpr)
  # Re-estimate IV on shuffled data
  perm_model <- ivreg(iv_formula, data = ajr_perm)
  perm_coefs[i] <- coef(perm_model)["avexpr"]
}

# We can calculate the p-values based on those results (two-sided)
p_val_perm <- mean(abs(perm_coefs) >= abs(observed_coef))

# The results are the following:
cat("Permutation Test Results:\n")
cat("Observed Coefficient:", observed_coef, "\n")
cat("Permutation p-value:", p_val_perm, "\n")
cat("Normal approx p-value:", summary(iv_model)$coefficients["avexpr", "Pr(>|t|)"], "\n")
```


### Q2(b). Bootstrap Confidence Intervals

Using bootstrap resampling of observations:

1. Generate bootstrap 2SLS estimates of the coefficient on $T_i$.

2. Construct three 95% confidence intervals:
    - **Efron percentile**
    - **Bias-corrected (BC—google this one.)**

3. Compare the three CIs:
    - Do they include zero?
    - Are they wider or narrower?
    - What does this imply about the sampling distribution?

### Q2(c). Conceptual: Permutation vs Bootstrap

Explain—precisely—what the **permutation test** and the **bootstrap** each measure.

What is held fixed? What is resampled?

Why do they answer conceptually different questions?

## Q3. Instrument Validity and Timing (10 pts)

### Q3.1 Causal Priority in AJR's Theory

**Why settler mortality must be causally prior to institutions:**

[Write your answer here]

Key points to address:

- The logic of instrumental variables requires that $Z \rightarrow T \rightarrow Y$ (no reverse causation)
- In AJR's theory, high settler mortality $\rightarrow$ extractive institutions $\rightarrow$ lower growth
- If institutions could affect mortality rates retroactively, the IV assumption fails
- The exclusion restriction requires mortality affects GDP ONLY through institutions

### Q3.2 Timing Problems

**Issues with timing of settler mortality measurements:**

[Write your answer here]

Address each IV assumption:

**Relevance (First-stage):**

- Why does measurement timing affect the strength of the first stage?
- Consider data availability and measurement error

**Exclusion Restriction:**

- If mortality was measured long after colonization, what other channels might exist?
- Could later mortality reflect economic conditions rather than cause them?

**Independence:**

- Are there confounders that affect both late-measured mortality and outcomes?
- Geographic or climatic factors?

### Q3.3 Assessment of AJR Results

Based on your analysis:

```{r ajr-assessment}

```

**Your assessment:**

[Write your answer here]

Consider:

- Is the first stage strong enough?
- Are the IV assumptions plausible given the timing issues?
- What are the main threats to validity?
- Would you believe the causal interpretation?

## Q4. Albouy's Critique of AJR (10 pts)

### Q4(a). Measurement Problems

**Key measurement issues raised by Albouy (2012):**

1. [Issue 1]:

2. [Issue 2]:

**Why they matter for IV validity:**

[Your answer]

### Q4(b). Violations of IV Assumptions

**Relevance:**

Albouy's argument:

[Your answer]

**Independence:**

Albouy's argument:

[Your answer]

**Exclusion Restriction:**

Albouy's argument:

[Your answer]

### Q4(c). Sensitivity and Data Corrections

After Albouy reconstructs and corrects the mortality data:

**Effects on:**

1. **First stage:** [Your answer - what happens to F-statistic and coefficient?]

2. **Reduced form:** [Your answer - does the relationship weaken?]

3. **2SLS estimates:** [Your answer - how do the causal estimates change?]

**What this reveals about stability:**

[Your answer - are the AJR findings robust or fragile?]

### Q4(d). Interpretation

**Do you believe the AJR conclusions still hold?**

[Your answer]

**Or do the methodological issues undermine the core causal claim?**

[Your answer]

**Justification:**

[Provide clear reasoning based on:

- The strength of Albouy's critique
- Your replication results
- The plausibility of IV assumptions
- The sensitivity of findings to data corrections]

# Problem 2: Regression Discontinuity Design

## Part A - Data Loading and Setup

```{r load-rd-data}
# Ensure RDD packages are loaded
library(rdrobust)
library(rddensity)
library(rdlocrand)
library(knitr)
library(kableExtra)

# Load RD data
data <- read.csv("CTV_2020_Sage.csv")

# Define outcome, running variable, and covariates
Y <- data$mv_incpartyfor1
X <- data$mv_incparty

covs <- data[, c("pibpc", "population", "numpar_candidates_eff",
                 "party_DEM_wonlag1_b1", "party_PSDB_wonlag1_b1",
                 "party_PT_wonlag1_b1", "party_PMDB_wonlag1_b1")]
covsnm <- c("GDP per capita", "Population", "No. Effective Parties",
            "DEM Victory t-1", "PSDB Victory t-1", "PT Victory t-1", "PMDB Victory t-1")

cat("RDD data loaded successfully!\n")
cat("Observations:", nrow(data), "\n")
cat("Running variable (X): Incumbent Party's Margin of Victory\n")
cat("Outcome variable (Y): Incumbent Party Victory at t+1\n")
```

## Part B - Falsification Analysis

### Density Test

```{r density-test}
# McCrary density test using rddensity
rddens <- rddensity(X)
summary(rddens)

# Plot the density
rdplotdensity(rddens, X = data$mv_incparty[!is.na(data$mv_incparty)],
              xlab = "Incumbent Party's Margin of Victory at t",
              ylab = "Estimated density")

# Create summary table for density test
density_results <- data.frame(
  Test = "Manipulation Test",
  Statistic = sprintf("%.4f", rddens$test$t_jk),
  `p-value` = sprintf("%.4f", rddens$test$p_jk),
  Bandwidth = sprintf("%.2f / %.2f", rddens$h$left, rddens$h$right),
  `N (Left)` = format(rddens$N$left, big.mark = ","),
  `N (Right)` = format(rddens$N$right, big.mark = ","),
  `Eff. N (Left)` = format(rddens$N$eff_left, big.mark = ","),
  `Eff. N (Right)` = format(rddens$N$eff_right, big.mark = ","),
  check.names = FALSE
)

kable(density_results,
      booktabs = TRUE,
      align = c("l", "r", "r", "c", "r", "r", "r", "r"),
      caption = "Manipulation Test: Continuity of Density at Threshold") %>%
  kable_styling(full_width = FALSE, position = "center")
```

*Note: Test statistic based on local polynomial density estimation with triangular kernel. Null hypothesis: No discontinuity in density at threshold (no manipulation). High p-value indicates no evidence of manipulation around the cutoff.*

**Interpretation:**

The density test checks whether there is evidence of manipulation around the threshold (margin of victory = 0). A significant discontinuity in the density would suggest that parties can manipulate their vote margins to barely win elections, which would violate the RD identifying assumptions.

### Covariate Balance Tests

```{r covariate-balance}
# Initialize lists to store results
balance_results <- list()

# Test for balance in covariates at the threshold
for(c in 1:ncol(covs)){
  cat("\n")
  rdr_cov <- rdrobust(covs[,c], X)
  balance_results[[c]] <- rdr_cov
  
  # Create RD plot for this covariate
  rdplot(covs[,c], X,
         y.label = covsnm[c],
         x.label = "Incumbent Party's Margin of Victory",
         x.lim = c(-30, 30),
         binselect = "qsmv")
}

# Create a summary table of balance tests
balance_table <- data.frame(
  Covariate = covsnm,
  Coefficient = sprintf("%.4f", sapply(balance_results, function(x) x$coef[1])),
  `Robust SE` = sprintf("%.4f", sapply(balance_results, function(x) x$se[3])),
  `t-statistic` = sprintf("%.3f", sapply(balance_results, function(x) x$z[3])),
  `p-value` = sprintf("%.3f", sapply(balance_results, function(x) x$pv[3])),
  `N (Left)` = format(sapply(balance_results, function(x) x$N_h[1]), big.mark = ","),
  `N (Right)` = format(sapply(balance_results, function(x) x$N_h[2]), big.mark = ","),
  check.names = FALSE
)

kable(balance_table,
      booktabs = TRUE,
      align = c("l", "r", "r", "r", "r", "r", "r"),
      caption = "Covariate Balance Tests at Threshold") %>%
  kable_styling(full_width = FALSE, position = "center")
```

*Note: Robust bias-corrected RD estimates using MSE-optimal bandwidth selection. Standard errors calculated using nearest-neighbor variance estimator.*

**Interpretation:**

Covariate balance tests check whether pre-treatment characteristics are continuous at the threshold. If covariates jump discontinuously at the cutoff, this suggests the treatment is not "as-if" randomly assigned, which would undermine the validity of the RD design.

## Part C - Outcome Analysis

### RD Plot

```{r rd-plot}
# Create RD plot of outcome variable
rdplot(Y, X,
       y.label = "Incumbent Party Victory at t+1",
       x.label = "Incumbent Party's Margin of Victory at t")
```

### Continuity-Based RDD Analysis

```{r rd-analysis-main}
# Main RDD estimate without covariates
rdr <- rdrobust(Y, X)

# RDD estimate with covariates
rdrcovs <- rdrobust(Y, X, covs = covs)

# Create a comprehensive results table
results_table <- data.frame(
  Specification = c("Without Covariates", "With Covariates"),
  Coefficient = sprintf("%.4f", c(rdr$coef[1], rdrcovs$coef[1])),
  `Conv. SE` = sprintf("%.4f", c(rdr$se[1], rdrcovs$se[1])),
  `Robust SE` = sprintf("%.4f", c(rdr$se[3], rdrcovs$se[3])),
  `p-value` = sprintf("%.3f", c(rdr$pv[3], rdrcovs$pv[3])),
  `95% CI` = sprintf("[%.3f, %.3f]", c(rdr$ci[3,1], rdrcovs$ci[3,1]), 
                                      c(rdr$ci[3,2], rdrcovs$ci[3,2])),
  `BW (L/R)` = sprintf("%.2f / %.2f", c(rdr$bws[1], rdrcovs$bws[1]), 
                                      c(rdr$bws[2], rdrcovs$bws[2])),
  `N (Left)` = format(c(rdr$N_h[1], rdrcovs$N_h[1]), big.mark = ","),
  `N (Right)` = format(c(rdr$N_h[2], rdrcovs$N_h[2]), big.mark = ","),
  check.names = FALSE
)

kable(results_table,
      booktabs = TRUE,
      align = c("l", "r", "r", "r", "r", "c", "c", "r", "r"),
      caption = "Regression Discontinuity Estimates: Effect of Incumbent Party Victory on Future Electoral Success") %>%
  kable_styling(full_width = FALSE, position = "center")

# Print detailed summary for reference
cat("\n=== DETAILED RDD ESTIMATES ===\n\n")
cat("--- Without Covariates ---\n")
print(summary(rdr))
cat("\n--- With Covariates ---\n")
print(summary(rdrcovs))
```

*Notes: Dependent variable is Incumbent Party Victory at t+1 (in percentage points). Robust bias-corrected confidence intervals and p-values reported. MSE-optimal bandwidth selection with triangular kernel. BW (L/R) shows left/right bandwidths. Covariates include GDP per capita, Population, No. Effective Parties, and DEM/PSDB/PT/PMDB Victory at t-1.*

**Interpretation:**

The RDD estimates show the causal effect of barely winning an election (vs. barely losing) on the probability of the incumbent party winning the next election. This tests the "incumbency curse" hypothesis - that winning may actually hurt a party's chances in the next election due to weak parties and term limits in Brazilian municipalities.

Compare the estimates with and without covariates. If they are similar, this provides additional evidence that the RD design is valid (covariates should not matter much if treatment is as-if random near the threshold).

## Part D - Local Randomization Approach

```{r rd-local-random}
# Window selection for local randomization
rdwin <- rdwinselect(X, covs, wmin = 0.05, wstep = 0.01, nwindows = 200,
                     seed = 765, plot = TRUE, quietly = TRUE)

# Use selected window (or manually choose)
w <- 0.15

# Randomization inference
rdrand <- rdrandinf(Y, X, wl = -w, wr = w, reps = 1000, seed = 765)

# Create local randomization results table
local_rand_table <- data.frame(
  Approach = c("Continuity-Based", "Local Randomization"),
  Window = c(sprintf("±%.2f / ±%.2f", rdr$bws[1], rdr$bws[2]),
             sprintf("±%.2f", w)),
  Coefficient = sprintf("%.4f", c(rdr$coef[1], rdrand$obs.stat)),
  `Robust SE` = c(sprintf("%.4f", rdr$se[3]), "—"),
  `p-value` = sprintf("%.3f", c(rdr$pv[3], rdrand$p.value)),
  `95% CI` = c(sprintf("[%.3f, %.3f]", rdr$ci[3,1], rdr$ci[3,2]), "—"),
  `N (Left)` = format(c(rdr$N_h[1], rdrand$sumstats[1]), big.mark = ","),
  `N (Right)` = format(c(rdr$N_h[2], rdrand$sumstats[2]), big.mark = ","),
  Method = c("Asymptotic", "Permutation"),
  check.names = FALSE
)

kable(local_rand_table,
      booktabs = TRUE,
      align = c("l", "c", "r", "r", "r", "c", "r", "r", "l"),
      caption = "Comparison of RDD Approaches: Continuity-Based vs. Local Randomization") %>%
  kable_styling(full_width = FALSE, position = "center")

cat("\n=== LOCAL RANDOMIZATION INFERENCE (Detailed) ===\n")
print(summary(rdrand))
```

*Notes: Dependent variable is Incumbent Party Victory at t+1 (in percentage points). Local randomization uses 1,000 permutations with fixed margins assumption. Continuity-based approach uses MSE-optimal bandwidth with robust bias-correction. Local randomization assumes as-if random assignment within the narrow window.*

**Interpretation:**

The local randomization approach assumes that units very close to the threshold (within a narrow window) are essentially randomly assigned to treatment. This provides an alternative inference method that doesn't rely on asymptotic approximations and may be more appropriate with discrete running variables.

## Part E - Assessment

**Key Findings:**

1. **Density Test:** [Interpret whether there is evidence of manipulation]

2. **Covariate Balance:** [Summarize whether covariates are balanced]

3. **RDD Estimates:** [State the main findings about the incumbency effect]

4. **Robustness:** [Assess whether estimates are stable across specifications]

**Overall Validity:**

[Your assessment of whether the RDD design is credible in this context and whether the findings support the "incumbency curse" hypothesis]
