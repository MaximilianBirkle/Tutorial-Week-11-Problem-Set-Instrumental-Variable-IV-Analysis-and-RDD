---
title: "Tutorial Week 11: Instrumental Variable (IV) Analysis and RDD"
author:
  - Maximilian Birkle
  - Daniel Lehmann
  - Henry Lucas
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    number_sections: true
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    code_folding: show
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  cache = FALSE
)
```

# Setup, Package Loading & Data Loading

```{r load-packages}
# Install packages if needed
packages <- c("AER", "haven", "dplyr", "ggplot2", "stargazer",
              "boot", "lmtest", "sandwich", "knitr", "kableExtra",
              "rdrobust", "rddensity", "rdlocrand")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}
```


```{r load-data}
# Load the AJR (2001) replication data
ajr_data_full <- haven::read_dta("acemoglu.dta")
cat("Observations:", nrow(ajr_data_full), "\n")
cat("Variables:", ncol(ajr_data_full), "\n")

```

# Problem 1: Instrumental Variable Analysis

## Q1. Writing and Estimating the IV Model (10 pts)

1. **Write down the two-equation IV system** (first stage and second stage).
    
    Define explicitly:
    
    - $Y_i$: outcome
    - $T_i$: endogenous regressor
    - $Z_i$: instrument
    - $X_i$: controls

2. Estimate the following:
    - **First stage:**
        
        $$T_i = \alpha_1 + \beta_1 Z_i + \mathbf{X}_i' \gamma_1 + \varepsilon_{1i}$$
        
    - **Reduced form:**
        
        $$Y_i = \alpha_2 + \delta_1 Z_i + \mathbf{X}_i' \gamma_2 + \varepsilon_{2i}$$
        
    - **2SLS:**
        
        $$Y_i = \alpha_3 + \beta_2 T_i + \mathbf{X}_i' \gamma_3 + \varepsilon_{3i}$$
        
3. Report and interpret:
    - The reduced-form coefficient on $Z_i$.
    - The first-stage coefficient on $Z_i$.
    - The **first-stage F-statistic**. Is it above the rule-of-thumb threshold of $\approx$ 10?
    - The 2SLS estimate of the effect of $T_i$ on $Y_i$. Is it statistically significant?
    
```{r q1_estimating_iv_model, results='asis'}

# Clean dataset with complete cases
ajr_data <- subset(ajr_data_full, baseco == 1)
  

# We define our control variables as a string for an easier formula building
controls <- "lat_abst + f_french + f_brit + sjlofr + catho80 + muslim80 + no_cpm80"

# First Stage: (Z_i -> T_i): We regress Endogeneous (avexpr) on Instrument (logem4) + Controls
fs_formula <- as.formula(paste("avexpr ~ logem4 +", controls))
fs_model <- lm(fs_formula, data = ajr_data)

# Reduced Form: (Z_i -> Y_i): We regress Outcome (logpgp95) on Instrument (logem4) + Controls
rf_formula <- as.formula(paste("logpgp95 ~ logem4 +", controls))
rf_model <- lm(rf_formula, data = ajr_data)

# 2SLS: (T_i -> Y_i): We regress Outcome (logpgp95) on Endogeneous (avexpr) + Controls
# For this stage, we have to use the ivreg-command. Since T_i is endogeneous (Y -> X),
# we need to run this command in order to isolate the specific variation in institutions
# caused by historical mortality rates and uses only that chunk of variation to explain GDP
iv_formula <- as.formula(paste("logpgp95 ~ avexpr +", controls, "| logem4 +", controls))
iv_model <- ivreg(iv_formula, data = ajr_data)

# For an instrumental variable to work, we have to verify that our instrumental variable Z_i
# is not a 'weak instrument'. Therefore, the instrument (Mortality, Z_i) must trigger a 
# significant change in the endogeneous variable (Institutions, T_i)
f_test <- linearHypothesis(fs_model, "logem4 = 0")
f_stat <- f_test$F[2]

# Output results using stargazer
stargazer(fs_model, rf_model, iv_model, 
          type = "latex", 
          title = "IV Analysis Results",
          column.labels = c("First Stage", "Reduced Form", "2SLS"),
          dep.var.labels = c("Exprop. Risk (T)", "Log GDP (Y)", "Log GDP (Y)"),
          covariate.labels = c("Log Settler Mortality (Z)", "Exprop. Risk (T)"),
          add.lines = list(c("First-Stage F-stat", round(f_stat, 2), "-", "-")))
```

**Discussion:**

The IV analysis reveals rather mixed results. Looking at the first stage (Column 1), we find that a one-unit increase in log settler mortality decreases institutional quality  by -0.385 points. While this is statistically significant, the first stage F-statistic of 5.3 falls well below the threshold of 10, indicating a weak instrument problem. This indicates that settler mortality does not predict institutions strongly enough to reliably isolate exogenous variation.

The reduced form (Column 2) shows that higher settler mortality is associated with lower GDP today (coefficient = -0.449, p < 0.001). This is the total effect of the instrument on the outcome, combining both the effect through institutions and any potential direct effects.

The 2SLS estimate (Column 3) suggests that a one-point improvement in institutional quality increases log GDP by 1.167, implying institutions have a large positive effect on economic development. However, this result must be interpreted with caution given the weak first stage. This weak instrument means we cannot confidently conclude that institutions cause higher GDP and that the relationship found may be spurious, driven by small-sample bias rather than a true causal relationship.

___

## Q2. Randomization and Resampling (10 pts)

### Q2(a). Permutation Test

1. **State the null hypothesis** being tested.

* $H_0$: Institutions have no causal effect on log GDP per Capita.

The null hypothesis, $H_{0}$ states  that the degree of protection against expropriation risk (the institution measure) has no causal influence ($\beta_{2}=0$) on a country's long-run economic outcome (log GDP per capita).

2. Conduct a **permutation test** for the IV coefficient:
    - Shuffle the endogenous variable (or fitted values) while holding other variables fixed.
    - Re-estimate the IV model for each permutation.
    - Construct the empirical null distribution.
    - Report the **permutation p-value**.

3. Compare this p-value to the **normal-approximation p-value** from your 2SLS output.
    
    Discuss any differences and what they imply for small-sample IV inference.
    
```{r q2_a_permutation_test}
# Permutation Test
set.seed(123)
n_perms <- 1000
observed_coef <- coef(iv_model)["avexpr"]
perm_coefs <- numeric(n_perms)

for (i in 1:n_perms) {
  # Shuffle the endogenous variable
  ajr_perm <- ajr_data
  ajr_perm$avexpr <- sample(ajr_data$avexpr)
  # Re-estimate IV on shuffled data
  perm_model <- ivreg(iv_formula, data = ajr_perm)
  perm_coefs[i] <- coef(perm_model)["avexpr"]
}

# Calculate the p-values based on those results (two-sided)
p_val_perm <- mean(abs(perm_coefs) >= abs(observed_coef))
p_val_normal <- summary(iv_model)$coefficients["avexpr", "Pr(>|t|)"]

# Create a summary table
perm_results <- data.frame(
  Statistic = c("Observed 2SLS Coefficient", 
                "Permutation p-value", 
                "Standard p-value",
                "Number of Permutations"),
  Value = c(sprintf("%.4f", observed_coef),
            sprintf("%.4f", p_val_perm),
            sprintf("%.4f", p_val_normal),
            format(n_perms, big.mark = ",")),
  check.names = FALSE
)

kable(perm_results,
      booktabs = TRUE,
      align = c("l", "r"),
      caption = "Permutation Test Results for IV Coefficient") %>%
  kable_styling(full_width = FALSE, position = "center")
```


**Discussion**

The comparison of the two estimates reveals a critical tension in the results. The point estimates for the 2SLS coefficient remain remarkably stable and virtually identical (1.167 vs. 1.1666). The primary concern arises instead from the large discrepancy between the $p$-values.

The standard $p$-value ($0.0028$), which is based on asymptotic (large-sample) assumptions (i.e., a normal sampling distribution), indicates that the observed coefficient estimate is highly significant and unlikely to be due to chance alone. However, the permutation $p$-value ($0.9470$), generated from 1,000 random shuffles that simulate a world with no causal effect between institutions and GDP per Capita, suggests the opposite. This high $p$-value indicates that approximately $94.7\%$ of the permuted coefficients are as large as or larger in absolute value than the observed coefficient. Therefore, under the null hypothesis of no relationship, we would very frequently observe coefficients as extreme as the one found.

This finding has important implications, as the weak instrument problem combined with small sample size means that the 2SLS estimator falsely suggests strong evidence for a causal effect when, the data are consistent with no effect at all. This casts serious doubt on core empirical claim driven by the combination of a small sample size and a weak first-stage instrument ($F=5.3$). 

___

### Q2(b). Bootstrap Confidence Intervals

Using bootstrap resampling of observations:

1. Generate bootstrap 2SLS estimates of the coefficient on $T_i$.

2. Construct three 95% confidence intervals:
    - **Efron percentile**
    - **Bias-corrected (BC—google this one.)**

3. Compare the three CIs:
    - Do they include zero?
    - Are they wider or narrower?
    - What does this imply about the sampling distribution?


```{r q2_b_bootstrap_confidence_intervals}
# Q2(b). Bootstrap Confidence Intervals
set.seed(456)
n_boot <- 1000 # number of bootstrap replications

# Function to compute 2SLS coefficient from bootstrap sample
# Resampling with replaement
boot_2sls <- function(data, indices) {
  boot_data <- data[indices, ]
  boot_model <- ivreg(iv_formula, data = boot_data) # Run the 2SLS regression on resanpled data
  return(coef(boot_model)["avexpr"])
}

# Run bootstrap 1,000 times 
boot_results <- boot(data = ajr_data, 
                     statistic = boot_2sls, 
                     R = n_boot)

# Storing all bootstrap coefficients
boot_coefs <- boot_results$t

# Calculate confidence intervals

# 1. Efron Percentile CI (assumes bootstrap distribution represents sampling distribution)
ci_percentile <- quantile(boot_coefs, probs = c(0.025, 0.975))
# Finds the 2.5th and 97.5th percentile that servrs as 95% CI


# 2. Bias-Corrected (BC) CI
# asdjusts percentile CI e.g. if bootstrap distribution is far from original estimate

# calculate bias correction factor => how many bootsrap coefficients are below observed coefficient
# if = 0.5 no correction is needed
z0 <- qnorm(mean(boot_coefs < observed_coef)) 
alpha <- c(0.025, 0.975)
z_alpha <- qnorm(alpha)
bc_probs <- pnorm(2 * z0 + z_alpha)
# Use the corrected percentile points to define final BC CI
ci_bc <- quantile(boot_coefs, probs = bc_probs)


# 3. Normal approximation CI (for comparison)
# Bootstrap's SE and build CI (= coefficient +/- 1.96 * Bootstrap SE)
boot_se <- sd(boot_coefs)
ci_normal <- observed_coef + c(-1, 1) * qnorm(0.975) * boot_se

# Create summary table
ci_comparison <- data.frame(
  Method = c("Normal Approximation", "Efron Percentile", "Bias-Corrected (BC)"),
  Lower_Bound = sprintf("%.4f", c(ci_normal[1], ci_percentile[1], ci_bc[1])),
  Upper_Bound = sprintf("%.4f", c(ci_normal[2], ci_percentile[2], ci_bc[2])),
  CI_Width = sprintf("%.4f", c(diff(ci_normal), diff(ci_percentile), diff(ci_bc))),
  Includes_Zero = c(ci_normal[1] <= 0 & ci_normal[2] >= 0,
                    ci_percentile[1] <= 0 & ci_percentile[2] >= 0,
                    ci_bc[1] <= 0 & ci_bc[2] >= 0)
)

kable(ci_comparison,
      booktabs = TRUE,
      align = c("l", "r", "r", "r", "c"),
      col.names = c("Method", "Lower Bound", "Upper Bound", "CI Width", "Includes Zero"),
      caption = "Bootstrap 95\\% Confidence Intervals for IV Coefficient") %>%
  kable_styling(full_width = FALSE, position = "center")

# Additional statistics table
boot_stats <- data.frame(
  Statistic = c("Observed 2SLS Coefficient",
                "Bootstrap Mean",
                "Bootstrap Bias",
                "Bootstrap SE",
                "Bootstrap Skewness",
                "Number of Bootstrap Replications"),
  Value = c(sprintf("%.4f", observed_coef),
            sprintf("%.4f", mean(boot_coefs)),
            sprintf("%.4f", mean(boot_coefs) - observed_coef),
            sprintf("%.4f", boot_se),
            sprintf("%.4f", mean(((boot_coefs - mean(boot_coefs)) / sd(boot_coefs))^3)),
            format(n_boot, big.mark = ","))
)

kable(boot_stats,
      booktabs = TRUE,
      align = c("l", "r"),
      caption = "Bootstrap Summary Statistics") %>%
  kable_styling(full_width = FALSE, position = "center")
```

**Discusion**

Looking at the results, we can see that the Normal Approximation (CI: [-13.56, 15.89]), as well as the Efron Percentile (CI: [-1.14, 4.88]) includes zero, suggesting that we cannot rule out the possibility of no causal effect. However, the Bias-Corrected CI excludes zero, suggesting that there may be a positive effect of institutions on GDP. 

The Normal Approximation CI is significantly wider (Width = 29.45), than Efron Percentile (Width = 6.02) and Bias-Corrected (BC) (Width = 5.90). 

The wide differences indicate that we should have serious concerns with the IV estimation. The bootstrap distribution appears to be heavily non-normal (skewness = -17.33), meaning the coefficient values are heavily skewed toward large negative numbers. This explains why the Normal Approximation CI is so much wider than the percentile based CIs. It assumes a symmetric, bell-shaped distribution when the actual distribution has a long left tail. This may result from the weak instrument problem (F = 5.3), since for a weak first stage, the 2SLS estimator becomes unstable in small samples, producing unreliable estimates that vary across bootstrap replications. This is further confirmed by the bootstrap SE (7.51). 

While the Bias-Corrected CI excludes zero [0.64, 6.54], therefore might suggest that there is a causal effect, this conflicts with the results from our permutatuion test that showed no statistical significance. The bootstrap's wide intervals and skewness show that the we cannot reliably reject the null hypothesis, as the data don't provide enough information.

--- 

### Q2(c). Conceptual: Permutation vs Bootstrap

**Explain—precisely—what the permutation test and the bootstrap each measure.** 

The permutation test tests the null hypothesis that institutions have no causal effect on GDP. To do that, it randomly shuffles the institutions variable across our 64 countries, abolishing any true relationship between institutions, while keeping everything else the same. The IV regression is then run again on the new composition of the data to estimate what coefficients result, if there is no effect. 

The bootstrap assumes the relationship to be real and asks how much it varies due to sampling randomness. For that, the all observations (countries) are resampled with replacement, creating 1,000 "fake" datasets that mimic drawing different samples from the same population (the 64 countries), where each resample preserves the actual relationships between mortality, institutions, and GDP. By re-estimating the IV model, on each resample, the resulting distribution shows where the true coefficient might fall, providing CIs to get a better grasp of estimation uncertainty. Although, this does not tell us whether the effect truly exists.


**What is held fixed? What is resampled?**

The permutation test only shuffles the institutions (breaking the Z→T→Y chain), while everything else stays fixed. The bootstrap resamples entire observations (rows) with replacement, preserving individual relationships.


**Why do they answer conceptually different questions?**

The Permutation Test asks: "Is there actually an effect?", while the bootstrap asks: "If there is an effect, how uncertain are we about its size?".

The permutation test shows we cannot reject the null hypothesis, as there is no evidence from our data that institutions affect GDP. It directly tests whether there is an effect.

The bootstrap assumes that there is an effect and tries to measure it. The results show wide CIs, extreme skewness and a large standard error reflecting instability in the estimator.

In summary, the permutation test gives more reliable inference, as it tests if there is an effect at all, while the bootstrap's conflicting results show we can't reliably estimate the relationship even if we assume one exists.


---

## Q3. Instrument Validity and Timing (10 pts)

### Q3.1 Explain why settler mortality must be causally prior to institutions in AJR’s theory.

Settler mortality must come before institutions in AJR's theory because the IV identification strategy requires a strict causal ordering. The instrument (settler mortality) can only affect the outcome (GDP) through its effect on institutions, not through any other channel. Historical evidence strongly supports this causal sequence. When European Colonizers arrived in the 17th-19th centuries, they encountered different disease environments with varying mortality rates. High mortality rates from diseases like yellow fever or malaria in places like West Africa made settlement impossible, leading colonizers to only establish extractive institutions focused on resource extraction. Low mortality rates in North America and Australia enabled mass settlement, and these settlers established strong property rights and rule of law.

The key takeaway from that reasoning is that colonizers made these institutional choices based on the initial disease environment, so before any institutions could exist. If institutions could affect mortality rates retroactively, then mortality rates would be endogenous, violating the IV assumption. As this is extremely unlikely, the temporal ordering ensures settler mortality was an exogenous shock determined purely by pre-existing epidemiological conditions, which influenced institutional development but could not itself be influenced by those institutions. This one-way causal path is what allows AJR to claim they have isolated the causal effect of institutions on development.

___

### Q3.2 Why does the timing of the settler mortality measurements (often recorded long after colonization) create problems for:
    
1. **Relevance**

Because the mortality data were recorded long after colonization, they may not reflect the disease environment that actually shaped institutional choices at the time. What we need is “mortality at first settlement”; what we have is mortality measured decades later, after medicine, colonial presence, and population structures had already changed. This mismatch introduces noise and can easily weaken the predictive link between mortality and institutions, which is consistent with our weak first stage.

2. **Exclusion restriction**

Late-recorded mortality may also capture post-colonial factors (like health investments, military activity, trade intensity) that directly influence modern GDP. If the measured mortality reflects these later developments, then it can affect GDP through channels other than institutions. In that case the exclusion restriction fails, since mortality would no longer influence income only through the institutional path.

3. **Independence**

If mortality is recorded long after settlement, it may pick up omitted variables that independently influence long-run development. For example, a region on a major trade route might receive better colonial medical infrastructure, reducing late-recorded mortality, while the same trade route also boosts modern GDP. That would make mortality correlated with unobservables that matter for income, violating independence. In short, the timing opens the door for mortality to become endogenous.

--- 

### Q3.3 Assessment of AJR Results: Do you find the AJR results convincing? Answer with reference to the IV assumptions and the estimated strength of the first stage.

Based on our IV analysis, the AJR results are not particularly convincing. The most immediate problem is the weak first stage, although settler mortality enters significantly, the first-stage F-statistic of around 5.3 is far below the usual threshold of 10, indicating that the instrument explains very little of the variation in institutional quality. This weak link makes the 2SLS estimates highly unstable, which is exactly what we observe in the permutation and bootstrap exercises. Both show extreme variability and results that are fully consistent with the null hypothesis of no causal effect. Beyond relevance, the exclusion restriction also appears fragile, because mortality was measured long after initial colonization, the recorded values likely incorporate post-colonial factors such as health interventions, military activity, or trade dynamics that directly influence long-run development, meaning mortality could affect GDP through channels other than institutions. Independence is questionable for similar reasons, since late-recorded mortality may reflect omitted geographic or historical characteristics that also matter for modern income, making the instrument plausibly endogenous. While the 2SLS point estimate is large and positive, our small-sample inference strongly suggests that this estimate cannot be interpreted as credible causal evidence. Taken together, the weak first stage, the timing issues surrounding the mortality data, and the poor performance of the instrument in our robustness checks imply that the AJR identification strategy does not provide strong statistical support for their causal claim about institutions and development.
___ 


## Q4. Albouy's Critique of AJR (10 pts)

### Q4(a). Measurement Problems

**Key measurement issues raised by Albouy (2012):**

One of the most damaging critiques is that AJR effectively make an "apples-to-oranges" comparison by mixing different types of mortality data. Albouy points out that they combine death rates from soldiers living in peaceful barracks with those on active military campaigns. This distinction is critical because historical evidence shows that soldiers on campaign faced drastically higher mortality—often between 66% and 2000% higher—caused by grueling travel, contaminated water, and exposure, rather than simply the local disease environment. Albouy discovers that AJR were more likely to use these inflated "campaign" rates for countries that are poor and have weak institutions today, while using the lower "barracks" rates for wealthier nations3. Consequently, the instrument ($Z$) appears to predict modern outcomes not because of a true historical causal link, but because the data was assembled in a way that artificially creates a correlation between high mortality and poor economic performance.

Perhaps even more concerning is that a majority of the dataset does not actually originate from the countries being analyzed. Albouy notes that for 36 of the 64 countries, AJR did not use local historical records; instead, they simply assigned mortality rates from neighboring countries based on their own assumptions about which nations shared similar disease environments. This methodology leads to significant errors, such as taking a mortality rate from a specific yellow fever epidemic in Mali and applying it to Angola and Uganda—countries thousands of kilometers away with vastly different climates and disease ecologies. In simple terms, this means the instrument ($Z$) is largely capturing the authors' best guesses rather than hard historical facts. Once Albouy strips away these conjectures or corrects the data, the strong variation in the instrument falls apart, and the link between settler mortality ($Z$) and institutions ($T$) essentially disappears.

### Q4(b). Violations of IV Assumptions

**Relevance** ($Cov(Z, T) \neq 0$)

Albouy argues that the strong relationship AJR found is largely an illusion caused by data errors. When he cleans up the dataset —by removing the made-up conjectured rates and stripping out the mismatched campaign data — the statistical link between mortality and institutions effectively evaporates. The instrument stops being a reliable predictor and becomes a weak instrument, meaning it no longer has enough power to explain why some countries have better institutions than others.


**Independence** ($Z \perp \epsilon$)

Albouy finds that the measurement error in the AJR data wasn't just random noise; instead it was systematic. He discovered a suspicious pattern where AJR were significantly more likely to use the inflated campaign mortality rates (from soldiers at war) for countries that are currently poor and have weak institutions. This implies the data selection process itself was biased/endogenous: the severity of the mortality rate was partly determined by the country's current economic status, creating a spurious correlation that violates independence.


**Exclusion Restriction** ($Z \rightarrow Y$ only through $T$)

Albouy argues that by relying on campaign rates, the instrument isn't just measuring germs — it is measuring the intensity of 19th-century warfare. Wars destroy infrastructure and human capital directly, which hurts GDP through channels having nothing to do with property rights institutions. Furthermore, Albouy notes that the results depend almost entirely on the "Neo-Europes" (US, Canada, New Zealand). If the instrument is just a proxy for being a Neo-Europe, it is likely capturing all the other advantages European settlers brought with them —like education, technology, and culture —rather than just their legal institutions, thereby breaking the exclusion restriction.


### Q4(c). Sensitivity and Data Corrections

When Albouy reconstructs the dataset by removing the "conjectured" mortality rates (those assigned from neighboring countries) and correcting the erroneous data points (like the Mali outlier), the results change as follows:

**Effects on:**

1. **First stage:** 
The relationship between settler mortality ($Z$) and institutions ($T$) collapses. When the conjectured rates are dropped and the Mali rate is corrected, the estimated coefficient for mortality ($\beta$) falls and becomes statistically insignificant, often indistinguishable from zero. This indicates that the instrument loses its predictive power once the measurement errors are addressed, leading to a failure of the relevance assumption.

2. **Reduced form:** 
The direct relationship between settler mortality and economic performance ($Y$) weakens substantially. Albouy notes that if the conjectured data are removed and controls for the data source (campaign vs. laborers) are added, the empirical relationship between mortality and the variables of interest virtually disappears.

3. **2SLS estimates:** 
Due to the weak first stage, the 2SLS point estimates become highly unstable and unreliable. The estimated coefficients often take on implausibly large values (e.g., implying massive GDP differences from small institutional changes) or even flip to being negative. Furthermore, the correct confidence regions (using the Anderson-Rubin statistic) often become unbounded (infinite), meaning the data is consistent with the causal effect being any value from negative infinity to positive infinity.

**What this reveals about stability:**
These findings reveal that the AJR results are exceptionally fragile and dependent on two specific data artifacts: the 36 conjectured mortality rates and the unadjusted campaign mortality rates. Albouy shows that when both robustness checks are applied simultaneously (removing conjectured rates and controlling for campaign sources), the relationship between mortality and institutions loses statistical significance even in the simplest models, and the coefficient on mortality actually switches signs in several specifications. Furthermore, without the conjectured data points, the robust relationship is shown to be driven almost entirely by the four "Neo-Europe" countries (USA, Canada, Australia, New Zealand); excluding them leaves a sample of only 13 observations where the theory finds no empirical support. 

### Q4(d). Interpretation

**Do you believe the AJR conclusions still hold?**

[Your answer]

**Or do the methodological issues undermine the core causal claim?**

[Your answer]

**Justification:**

[Provide clear reasoning based on:

- The strength of Albouy's critique
- Your replication results
- The plausibility of IV assumptions
- The sensitivity of findings to data corrections]

___

# Problem 2: Regression Discontinuity Design

Note: Due to the fact that the authors provided their replication code on their github, we used that for the analysis below.

## Part A - Data Loading and Setup

```{r load-rd-data}
# Ensure RDD packages are loaded
library(rdrobust)
library(rddensity)
library(rdlocrand)
library(knitr)
library(kableExtra)

# Load RD data
data <- read.csv("CTV_2020_Sage.csv")

# Define outcome, running variable, and covariates
Y <- data$mv_incpartyfor1
X <- data$mv_incparty

covs <- data[, c("pibpc", "population", "numpar_candidates_eff",
                 "party_DEM_wonlag1_b1", "party_PSDB_wonlag1_b1",
                 "party_PT_wonlag1_b1", "party_PMDB_wonlag1_b1")]
covsnm <- c("GDP per capita", "Population", "No. Effective Parties",
            "DEM Victory t-1", "PSDB Victory t-1", "PT Victory t-1", "PMDB Victory t-1")

cat("RDD data loaded successfully!\n")
cat("Observations:", nrow(data), "\n")
cat("Running variable (X): Incumbent Party's Margin of Victory\n")
cat("Outcome variable (Y): Incumbent Party Victory at t+1\n")
```

___

## Part B - Falsification Analysis

### Density Test

```{r density-test}
# McCrary density test using rddensity
rddens <- rddensity(X)
summary(rddens)

# Plot the density
rdplotdensity(rddens, X = data$mv_incparty[!is.na(data$mv_incparty)],
              xlab = "Incumbent Party's Margin of Victory at t",
              ylab = "Estimated density")

# Create summary table for density test
density_results <- data.frame(
  Test = "Manipulation Test",
  Statistic = sprintf("%.4f", rddens$test$t_jk),
  `p-value` = sprintf("%.4f", rddens$test$p_jk),
  Bandwidth = sprintf("%.2f / %.2f", rddens$h$left, rddens$h$right),
  `N (Left)` = format(rddens$N$left, big.mark = ","),
  `N (Right)` = format(rddens$N$right, big.mark = ","),
  `Eff. N (Left)` = format(rddens$N$eff_left, big.mark = ","),
  `Eff. N (Right)` = format(rddens$N$eff_right, big.mark = ","),
  check.names = FALSE
)

kable(density_results,
      booktabs = TRUE,
      align = c("l", "r", "r", "c", "r", "r", "r", "r"),
      caption = "Manipulation Test: Continuity of Density at Threshold") %>%
  kable_styling(full_width = FALSE, position = "center")
```

*Note: Test statistic based on local polynomial density estimation with triangular kernel. Null hypothesis: No discontinuity in density at threshold (no manipulation). High p-value indicates no evidence of manipulation around the cutoff.*

**Interpretation:**

The density test checks whether there is evidence of manipulation around the threshold (margin of victory = 0). A significant discontinuity in the density would suggest that parties can manipulate their vote margins to barely win elections, which would violate the RD identifying assumptions.

### Covariate Balance Tests

```{r covariate-balance}
# Initialize lists to store results
balance_results <- list()

# Test for balance in covariates at the threshold
for(c in 1:ncol(covs)){
  cat("\n")
  rdr_cov <- rdrobust(covs[,c], X)
  balance_results[[c]] <- rdr_cov
  
  # Create RD plot for this covariate
  rdplot(covs[,c], X,
         y.label = covsnm[c],
         x.label = "Incumbent Party's Margin of Victory",
         x.lim = c(-30, 30),
         binselect = "qsmv")
}

# Create a summary table of balance tests
balance_table <- data.frame(
  Covariate = covsnm,
  Coefficient = sprintf("%.4f", sapply(balance_results, function(x) x$coef[1])),
  `Robust SE` = sprintf("%.4f", sapply(balance_results, function(x) x$se[3])),
  `t-statistic` = sprintf("%.3f", sapply(balance_results, function(x) x$z[3])),
  `p-value` = sprintf("%.3f", sapply(balance_results, function(x) x$pv[3])),
  `N (Left)` = format(sapply(balance_results, function(x) x$N_h[1]), big.mark = ","),
  `N (Right)` = format(sapply(balance_results, function(x) x$N_h[2]), big.mark = ","),
  check.names = FALSE
)

kable(balance_table,
      booktabs = TRUE,
      align = c("l", "r", "r", "r", "r", "r", "r"),
      caption = "Covariate Balance Tests at Threshold") %>%
  kable_styling(full_width = FALSE, position = "center")
```

*Note: Robust bias-corrected RD estimates using MSE-optimal bandwidth selection. Standard errors calculated using nearest-neighbor variance estimator.*

**Interpretation:**

Covariate balance tests check whether pre-treatment characteristics are continuous at the threshold. If covariates jump discontinuously at the cutoff, this suggests the treatment is not "as-if" randomly assigned, which would undermine the validity of the RD design.

___

## Part C - Outcome Analysis

### RD Plot

```{r rd-plot}
# Create RD plot of outcome variable
rdplot(Y, X,
       y.label = "Incumbent Party Victory at t+1",
       x.label = "Incumbent Party's Margin of Victory at t")
```

### Continuity-Based RDD Analysis

```{r rd-analysis-main}
# Main RDD estimate without covariates
rdr <- rdrobust(Y, X)

# RDD estimate with covariates
rdrcovs <- rdrobust(Y, X, covs = covs)

# Create a comprehensive results table
results_table <- data.frame(
  Specification = c("Without Covariates", "With Covariates"),
  Coefficient = sprintf("%.4f", c(rdr$coef[1], rdrcovs$coef[1])),
  `Conv. SE` = sprintf("%.4f", c(rdr$se[1], rdrcovs$se[1])),
  `Robust SE` = sprintf("%.4f", c(rdr$se[3], rdrcovs$se[3])),
  `p-value` = sprintf("%.3f", c(rdr$pv[3], rdrcovs$pv[3])),
  `95% CI` = sprintf("[%.3f, %.3f]", c(rdr$ci[3,1], rdrcovs$ci[3,1]), 
                                      c(rdr$ci[3,2], rdrcovs$ci[3,2])),
  `BW (L/R)` = sprintf("%.2f / %.2f", c(rdr$bws[1], rdrcovs$bws[1]), 
                                      c(rdr$bws[2], rdrcovs$bws[2])),
  `N (Left)` = format(c(rdr$N_h[1], rdrcovs$N_h[1]), big.mark = ","),
  `N (Right)` = format(c(rdr$N_h[2], rdrcovs$N_h[2]), big.mark = ","),
  check.names = FALSE
)

kable(results_table,
      booktabs = TRUE,
      align = c("l", "r", "r", "r", "r", "c", "c", "r", "r"),
      caption = "Regression Discontinuity Estimates: Effect of Incumbent Party Victory on Future Electoral Success") %>%
  kable_styling(full_width = FALSE, position = "center")

# Print detailed summary for reference
cat("\n=== DETAILED RDD ESTIMATES ===\n\n")
cat("--- Without Covariates ---\n")
print(summary(rdr))
cat("\n--- With Covariates ---\n")
print(summary(rdrcovs))
```

*Notes: Dependent variable is Incumbent Party Victory at t+1 (in percentage points). Robust bias-corrected confidence intervals and p-values reported. MSE-optimal bandwidth selection with triangular kernel. BW (L/R) shows left/right bandwidths. Covariates include GDP per capita, Population, No. Effective Parties, and DEM/PSDB/PT/PMDB Victory at t-1.*

**Interpretation:**

The RDD estimates show the causal effect of barely winning an election (vs. barely losing) on the probability of the incumbent party winning the next election. This tests the "incumbency curse" hypothesis - that winning may actually hurt a party's chances in the next election due to weak parties and term limits in Brazilian municipalities.

Compare the estimates with and without covariates. If they are similar, this provides additional evidence that the RD design is valid (covariates should not matter much if treatment is as-if random near the threshold).

___

## Part D - Local Randomization Approach

```{r rd-local-random}
# Window selection for local randomization
rdwin <- rdwinselect(X, covs, wmin = 0.05, wstep = 0.01, nwindows = 200,
                     seed = 765, plot = TRUE, quietly = TRUE)

# Use selected window (or manually choose)
w <- 0.15

# Randomization inference
rdrand <- rdrandinf(Y, X, wl = -w, wr = w, reps = 1000, seed = 765)

# Create local randomization results table
local_rand_table <- data.frame(
  Approach = c("Continuity-Based", "Local Randomization"),
  Window = c(sprintf("±%.2f / ±%.2f", rdr$bws[1], rdr$bws[2]),
             sprintf("±%.2f", w)),
  Coefficient = sprintf("%.4f", c(rdr$coef[1], rdrand$obs.stat)),
  `Robust SE` = c(sprintf("%.4f", rdr$se[3]), "—"),
  `p-value` = sprintf("%.3f", c(rdr$pv[3], rdrand$p.value)),
  `95% CI` = c(sprintf("[%.3f, %.3f]", rdr$ci[3,1], rdr$ci[3,2]), "—"),
  `N (Left)` = format(c(rdr$N_h[1], rdrand$sumstats[1]), big.mark = ","),
  `N (Right)` = format(c(rdr$N_h[2], rdrand$sumstats[2]), big.mark = ","),
  Method = c("Asymptotic", "Permutation"),
  check.names = FALSE
)

kable(local_rand_table,
      booktabs = TRUE,
      align = c("l", "c", "r", "r", "r", "c", "r", "r", "l"),
      caption = "Comparison of RDD Approaches: Continuity-Based vs. Local Randomization") %>%
  kable_styling(full_width = FALSE, position = "center")

cat("\n=== LOCAL RANDOMIZATION INFERENCE (Detailed) ===\n")
print(summary(rdrand))
```

*Notes: Dependent variable is Incumbent Party Victory at t+1 (in percentage points). Local randomization uses 1,000 permutations with fixed margins assumption. Continuity-based approach uses MSE-optimal bandwidth with robust bias-correction. Local randomization assumes as-if random assignment within the narrow window.*

**Interpretation:**

The local randomization approach assumes that units very close to the threshold (within a narrow window) are essentially randomly assigned to treatment. This provides an alternative inference method that doesn't rely on asymptotic approximations and may be more appropriate with discrete running variables.

## Part E - Assessment

**Key Findings:**

1. **Density Test:** [Interpret whether there is evidence of manipulation]

2. **Covariate Balance:** [Summarize whether covariates are balanced]

3. **RDD Estimates:** [State the main findings about the incumbency effect]

4. **Robustness:** [Assess whether estimates are stable across specifications]

**Overall Validity:**

[Your assessment of whether the RDD design is credible in this context and whether the findings support the "incumbency curse" hypothesis]
